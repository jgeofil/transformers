{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.16.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.37.2)\n",
      "Requirement already satisfied: tokenizers in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.15.1)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (8.1.1)\n",
      "Requirement already satisfied: tensorflow[and-cuda] in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (4.23.4)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.15.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.2.5.6 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.5.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.2.142 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.142)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.2.140 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.2.140 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.2.140 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.4.25 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (8.9.4.25)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.8.103 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (11.0.8.103)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.3.141 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (10.3.3.141)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.5.2.141 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (11.5.2.141)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.2.141 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.1.2.141)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.16.5 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.16.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.2.140 in ./.venv/lib/python3.10/site-packages (from tensorflow[and-cuda]->-r requirements.txt (line 1)) (12.2.140)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in ./.venv/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in ./.venv/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.42.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.17.2)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.venv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow[and-cuda]->-r requirements.txt (line 1)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, gzip, os\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('data')\n",
    "TEMP_FILE = os.path.join(DATA_DIR, 'temp.txt')\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "class SpecialTokens:\n",
    "    PAD = \"[PAD]\"\n",
    "    START = \"[STA]\"\n",
    "    END = \"[END]\"\n",
    "    UNK = \"[UNK]\"\n",
    "    TOKENS = [PAD, START, END, UNK]\n",
    "    TOKEN_TO_ID = {token: i for i, token in enumerate(TOKENS)}\n",
    "    TUPLES = [(token, i) for i, token in enumerate(TOKENS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=SpecialTokens.TOKENS,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(BPE(\n",
    "    unk_token=SpecialTokens.UNK,\n",
    "    end_of_word_suffix=SpecialTokens.END\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[STA] $A [END]\",\n",
    "    special_tokens=SpecialTokens.TUPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(os.path.join(DATA_DIR, \"*.txt.gz\"))\n",
    "os.remove(TEMP_FILE)\n",
    "\n",
    "for file in text_files:\n",
    "    with gzip.open(file, \"rt\") as f:\n",
    "        with open(TEMP_FILE, \"a\") as f2:\n",
    "            f2.write(f.read().replace(\"\\n\", \" \").replace(\"\\r\", \"\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(['data/temp.txt'], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534152\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jeremygf/domains-app-alpha\")\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb3cac42b04ca7afef65469c4b7d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    'ids': tokenizer.encode(x['text']).ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f99a8378b6145db9cab358302ec5374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    'input_ids': x['ids'][:-1],\n",
    "    'target_ids': x['ids'][1:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'ids', 'input_ids', 'target_ids'],\n",
       "    num_rows: 534152\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='tokenizer.json')\n",
    "tokenizer.pad_token = SpecialTokens.PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 1024\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LEN = 31\n",
    "EMBED_DIM = 96\n",
    "NUM_HEADS = 24\n",
    "FEED_FORWARD_DIM = 96\n",
    "NUM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_dataset = dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\"],\n",
    "    label_cols=[\"target_ids\"],\n",
    "    batch_size=BATCH,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "\n",
    "class CharTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size: int, sequence_length: int, num_layers: int, embedding_dim: int, \n",
    "        num_heads: int, intermediate_dim: int) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = TokenAndPositionEmbedding(\n",
    "            vocabulary_size=vocab_size,\n",
    "            sequence_length=sequence_length,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        self.decoders = [\n",
    "            TransformerDecoder(\n",
    "                num_heads=num_heads, \n",
    "                intermediate_dim=intermediate_dim,\n",
    "                dropout=0.1,\n",
    "                activation='gelu'\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='relu')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x)\n",
    "        return self.dense(x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "522/522 [==============================] - 108s 186ms/step - loss: 3.2127 - perplexity: 47.6775\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 09:39:03.793875: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11649731160769717573\n",
      "2024-02-06 09:39:03.794068: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15495456782289674620\n",
      "2024-02-06 09:39:03.794119: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10014077403232570392\n",
      "2024-02-06 09:39:03.794128: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 5551772001302607106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 98s 186ms/step - loss: 2.9521 - perplexity: 112.2612\n",
      "Epoch 3/100\n",
      "522/522 [==============================] - 98s 187ms/step - loss: 2.9749 - perplexity: 134.9536\n",
      "Epoch 4/100\n",
      "522/522 [==============================] - 96s 183ms/step - loss: 2.9719 - perplexity: 267.6299\n",
      "Epoch 5/100\n",
      "522/522 [==============================] - 96s 183ms/step - loss: 2.9642 - perplexity: 208.6354\n",
      "Epoch 6/100\n",
      "522/522 [==============================] - 96s 183ms/step - loss: 2.9567 - perplexity: 175.2582\n",
      "Epoch 7/100\n",
      "522/522 [==============================] - 96s 183ms/step - loss: 2.9712 - perplexity: 129.9656\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 96s 183ms/step - loss: 2.9690 - perplexity: 126.1075\n",
      "Epoch 9/100\n",
      "522/522 [==============================] - 97s 185ms/step - loss: 2.9675 - perplexity: 99.6855\n",
      "Epoch 10/100\n",
      "522/522 [==============================] - 99s 189ms/step - loss: 2.9662 - perplexity: 82.9400\n",
      "Epoch 11/100\n",
      "522/522 [==============================] - 97s 185ms/step - loss: 2.9647 - perplexity: 59.4367\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 99s 189ms/step - loss: 2.9631 - perplexity: 53.3588\n",
      "Epoch 13/100\n",
      " 48/522 [=>............................] - ETA: 1:27 - loss: 2.9651 - perplexity: 73.2684"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/git/hackroulette/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(text_ds, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('char_transformer_32_24h.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_packer = StartEndPacker(\n",
    "    MAX_SEQ_LEN,\n",
    "    start_value=SpecialTokens.START,\n",
    "    pad_value=SpecialTokens.PADDING,\n",
    "    return_padding_mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_prompt(prompt: str) -> tf.Tensor:\n",
    "    chars = tf.strings.unicode_split(prompt, input_encoding='UTF-8')\n",
    "    padded = prompt_packer(chars)\n",
    "    tokens = ids_from_chars(padded)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextt(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index-1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "[b'[STA][END][PAD]ii[END][PAD][END][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]t[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = ''\n",
    "prompt_length = len(prompt)\n",
    "prompt_tokens = pack_prompt([prompt])\n",
    "\n",
    "sampler = keras_nlp.samplers.TopPSampler(0.5)\n",
    "output_tokens = sampler(\n",
    "    next=nextt,\n",
    "    prompt=prompt_tokens,\n",
    "    index=prompt_length+1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = chars_from_ids(output_tokens)\n",
    "txt=tf.strings.reduce_join(txt, axis=-1).numpy()\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
