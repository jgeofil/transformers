{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 10:01:17.414370: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 10:01:17.634644: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 10:01:17.634699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 10:01:17.669428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 10:01:17.748845: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 10:01:18.734149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob, gzip, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1b5ea144ae45f09ea785b6c5dea9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069f0bc827d245c19e07e52a7e7fb0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d6aff3e90249ecab5460a06aaca218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534152\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jeremygf/domains-app-alpha\")\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        super(LayerNorm, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer='zeros', trainable=True)\n",
    "        super(LayerNorm, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
    "        x = (x - mean) * tf.math.rsqrt(variance + self.epsilon)\n",
    "        x = self.gamma * x + self.beta\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNorm()\n",
    "    \n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x, \n",
    "            value=x, \n",
    "            use_causal_mask=True\n",
    "        )\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, feedforward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = tf.keras.layers.Dense(feedforward_dim, activation='gelu')\n",
    "        self.d2 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.layer_norm = LayerNorm()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d2(self.d1(x))\n",
    "        x = x + self.dropout(x)\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, embedding_dim, num_heads, feedforward_dim, dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, embedding_dim=embedding_dim)\n",
    "\n",
    "    self.ffn = FeedForward(embedding_dim=embedding_dim, feedforward_dim=feedforward_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    return x    \n",
    "    \n",
    "    \n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=MAX_SEQ_LEN, depth=embedding_dim)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n",
    "\n",
    "class CharTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, embedding_dim: int, \n",
    "        num_heads: int, intermediate_dim: int) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = PositionalEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.decoders = [\n",
    "            DecoderLayer(embedding_dim, num_heads, intermediate_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='relu')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embeddings(inputs)  \n",
    "\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x)\n",
    "        return self.dense(x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 10:02:11.010343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.125084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.125120: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.127840: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.127873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.127886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.274327: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.274450: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.274460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-13 10:02:11.274517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-13 10:02:11.274535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "BATCH = 256\n",
    "EPOCHS = 10\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, \n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "tf_train = ddict['train'].to_tf_dataset(\n",
    "    columns=\"input_ids\",\n",
    "    label_cols=\"target_ids\",\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_test = ddict['test'].to_tf_dataset(\n",
    "    columns=\"input_ids\",\n",
    "    label_cols=\"target_ids\",\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "      \"d_model\": self.d_model,\n",
    "      \"warmup_steps\": self.warmup_steps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 10:06:07.704767: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBED_DIM = 256\n",
    "\n",
    "NUM_HEADS = 4\n",
    "FEED_FORWARD_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(EMBED_DIM)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model = CharTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=FEED_FORWARD_DIM\n",
    ")\n",
    "\n",
    "model.compile(loss=masked_loss, metrics=[masked_accuracy], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1670/1670 [==============================] - 100s 60ms/step - loss: 4.1581 - masked_accuracy: 0.2881 - val_loss: 4.2989 - val_masked_accuracy: 0.2769\n",
      "Epoch 2/10\n",
      "1670/1670 [==============================] - 95s 57ms/step - loss: 4.1394 - masked_accuracy: 0.2898 - val_loss: 4.3002 - val_masked_accuracy: 0.2765\n",
      "Epoch 3/10\n",
      "1670/1670 [==============================] - 96s 57ms/step - loss: 4.1227 - masked_accuracy: 0.2918 - val_loss: 4.3028 - val_masked_accuracy: 0.2766\n",
      "Epoch 4/10\n",
      "1670/1670 [==============================] - 97s 58ms/step - loss: 4.1068 - masked_accuracy: 0.2933 - val_loss: 4.3052 - val_masked_accuracy: 0.2768\n",
      "Epoch 5/10\n",
      "1670/1670 [==============================] - 98s 59ms/step - loss: 4.0917 - masked_accuracy: 0.2953 - val_loss: 4.3071 - val_masked_accuracy: 0.2766\n",
      "Epoch 6/10\n",
      "1670/1670 [==============================] - 92s 55ms/step - loss: 4.0779 - masked_accuracy: 0.2968 - val_loss: 4.3104 - val_masked_accuracy: 0.2758\n",
      "Epoch 7/10\n",
      "1670/1670 [==============================] - 96s 58ms/step - loss: 4.0648 - masked_accuracy: 0.2980 - val_loss: 4.3123 - val_masked_accuracy: 0.2761\n",
      "Epoch 8/10\n",
      "1670/1670 [==============================] - 94s 56ms/step - loss: 4.0522 - masked_accuracy: 0.2998 - val_loss: 4.3201 - val_masked_accuracy: 0.2759\n",
      "Epoch 9/10\n",
      "1670/1670 [==============================] - 94s 57ms/step - loss: 4.0407 - masked_accuracy: 0.3012 - val_loss: 4.3221 - val_masked_accuracy: 0.2764\n",
      "Epoch 10/10\n",
      "1670/1670 [==============================] - 104s 62ms/step - loss: 4.0290 - masked_accuracy: 0.3024 - val_loss: 4.3289 - val_masked_accuracy: 0.2740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1d477be850>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train, \n",
    "    validation_data=tf_test, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard_callback] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchar_transformer.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/transformers/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/git/transformers/.venv/lib/python3.11/site-packages/keras/src/optimizers/schedules/learning_rate_schedule.py:83\u001b[0m, in \u001b[0;36mLearningRateSchedule.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@abc\u001b[39m\u001b[38;5;241m.\u001b[39mabstractmethod\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate schedule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust override `get_config()` in order to be serializable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable."
     ]
    }
   ],
   "source": [
    "model.save(\"char_transformer.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ''\n",
    "prompt_length = len(tokenizer.encode(prompt))-1\n",
    "print(prompt_length)\n",
    "prompt = tokenizer.encode(prompt, padding='max_length', max_length=MAX_SEQ_LEN, return_tensors='tf')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextt(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index-1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sampler \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTopPSampler(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m sampler(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mnext\u001b[39m\u001b[38;5;241m=\u001b[39mnextt,\n\u001b[1;32m      5\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m      6\u001b[0m     index\u001b[38;5;241m=\u001b[39mprompt_length,  \u001b[38;5;66;03m# Start sampling immediately after the [BOS] token.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp'"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "sampler = keras_nlp.samplers.TopPSampler(0.5)\n",
    "output_tokens = sampler(\n",
    "    next=nextt,\n",
    "    prompt=prompt,\n",
    "    index=prompt_length,  # Start sampling immediately after the [BOS] token.\n",
    ")[0]\n",
    "print(output_tokens)\n",
    "print(tokenizer.decode(output_tokens).split('[END]'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
